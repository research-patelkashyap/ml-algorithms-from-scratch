{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7d30c5",
   "metadata": {},
   "source": [
    "## Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc950fe",
   "metadata": {},
   "source": [
    "**Gradient of Cost Function:**\n",
    "\n",
    "OLS:\n",
    "$ \\nabla_\\theta J(\\theta) = \\frac{2}{m} X^\\top (X\\theta - y) $\n",
    "\n",
    "Lasso Regression:\n",
    "$ \\nabla_\\theta J(\\theta) = \\frac{2}{m} X^\\top (X\\theta - y) \\;+\\; \\lambda \\, \\text{sign}(\\theta^*) $\n",
    "\n",
    "Ridge Regression:\n",
    "$ \\nabla_\\theta J(\\theta) = \\frac{2}{m} X^\\top (X\\theta - y) \\;+\\; 2\\lambda \\theta^* $\n",
    "\n",
    "ElasticNet:\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{m} X^\\top (X\\theta - y) \n",
    "\\;+\\; \\lambda \\alpha \\, \\text{sign}(\\theta^*) \n",
    "\\;+\\; 2\\lambda (1-\\alpha) \\theta^*\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0e56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbd6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=0.01,\n",
    "        n_iters=1000,\n",
    "        batch_size=None,\n",
    "        regularization=False,\n",
    "        regularization_method=None,\n",
    "        lambda_=1.0,\n",
    "        alpha=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        lr: learning rate\n",
    "        n_iters: number of epochs\n",
    "        batch_size:\n",
    "            - None  -> Batch GD (full dataset)\n",
    "            - 1     -> Stochastic GD\n",
    "            - k     -> Mini-batch GD of size k\n",
    "        regularization: regularization technique\n",
    "        regularization_method:\n",
    "            - L1: Lasso\n",
    "            - L2: Ridge\n",
    "            - Elastic: ElasticNet\n",
    "        lambda_: regularization strength\n",
    "        alpha: ElasticNet mixing parameter\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.theta = None\n",
    "        self.regularization = regularization\n",
    "        self.regularization_method = regularization_method\n",
    "        self.lambda_ = lambda_\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X_b = np.hstack([ones, X])\n",
    "\n",
    "        n_samples, n_features = X_b.shape\n",
    "        self.theta = np.zeros((n_features, 1))\n",
    "\n",
    "        batch_size = self.batch_size or n_samples\n",
    "\n",
    "        for epoch in range(self.n_iters):\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_b_shuffled = X_b[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                X_batch = X_b_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "\n",
    "                y_pred = X_batch @ self.theta\n",
    "\n",
    "                grad = (2 / X_batch.shape[0]) * (X_batch.T @ (y_pred - y_batch))\n",
    "\n",
    "                if self.regularization:\n",
    "                    theta_reg = self.theta.copy()\n",
    "                    theta_reg[0] = 0\n",
    "\n",
    "                    if self.regularization_method == 'L1':\n",
    "                        grad += self.lambda_ * np.sign(theta_reg)\n",
    "                    elif self.regularization_method == 'L2':\n",
    "                        grad += self.lambda_ * 2 * theta_reg\n",
    "                    elif self.regularization_method == 'Elastic':\n",
    "                        grad += (self.lambda_ * self.alpha * np.sign(theta_reg) + self.lambda_ * (1 - self.alpha) * 2 * theta_reg)\n",
    "\n",
    "                self.theta -= self.lr * grad\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                y_full_pred = X_b @ self.theta\n",
    "                mse = np.mean((y_full_pred - y) ** 2)\n",
    "                print(f\"Epoch {epoch}/{self.n_iters} — Train MSE: {mse:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X_b = np.hstack([ones, X])\n",
    "        return X_b @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c93315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c099b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896788ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fab8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30754dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (16512, 8) (16512, 1)\n",
      "Test shape: (4128, 8) (4128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a4ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 — Train MSE: 5.4312\n",
      "Epoch 100/1000 — Train MSE: 0.7066\n",
      "Epoch 200/1000 — Train MSE: 0.5950\n",
      "Epoch 300/1000 — Train MSE: 0.5731\n",
      "Epoch 400/1000 — Train MSE: 0.5583\n",
      "Epoch 500/1000 — Train MSE: 0.5476\n",
      "Epoch 600/1000 — Train MSE: 0.5398\n",
      "Epoch 700/1000 — Train MSE: 0.5341\n",
      "Epoch 800/1000 — Train MSE: 0.5299\n",
      "Epoch 900/1000 — Train MSE: 0.5268\n",
      "Test MSE: 0.5545795065308549\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionGD()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ef1131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 — Train MSE: 5.4312\n",
      "Epoch 100/1000 — Train MSE: 1.0587\n",
      "Epoch 200/1000 — Train MSE: 0.9522\n",
      "Epoch 300/1000 — Train MSE: 0.9454\n",
      "Epoch 400/1000 — Train MSE: 0.9476\n",
      "Epoch 500/1000 — Train MSE: 0.9504\n",
      "Epoch 600/1000 — Train MSE: 0.9476\n",
      "Epoch 700/1000 — Train MSE: 0.9491\n",
      "Epoch 800/1000 — Train MSE: 0.9508\n",
      "Epoch 900/1000 — Train MSE: 0.9465\n",
      "Test MSE: 0.9369758678060054\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionGD(regularization=True, regularization_method='L1')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76b1e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 — Train MSE: 5.4312\n",
      "Epoch 100/1000 — Train MSE: 0.8865\n",
      "Epoch 200/1000 — Train MSE: 0.8081\n",
      "Epoch 300/1000 — Train MSE: 0.8065\n",
      "Epoch 400/1000 — Train MSE: 0.8065\n",
      "Epoch 500/1000 — Train MSE: 0.8065\n",
      "Epoch 600/1000 — Train MSE: 0.8065\n",
      "Epoch 700/1000 — Train MSE: 0.8065\n",
      "Epoch 800/1000 — Train MSE: 0.8065\n",
      "Epoch 900/1000 — Train MSE: 0.8065\n",
      "Test MSE: 0.8021077157095555\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionGD(regularization=True, regularization_method='L2')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40021d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 — Train MSE: 5.4312\n",
      "Epoch 100/1000 — Train MSE: 0.9714\n",
      "Epoch 200/1000 — Train MSE: 0.8890\n",
      "Epoch 300/1000 — Train MSE: 0.8840\n",
      "Epoch 400/1000 — Train MSE: 0.8860\n",
      "Epoch 500/1000 — Train MSE: 0.8835\n",
      "Epoch 600/1000 — Train MSE: 0.8847\n",
      "Epoch 700/1000 — Train MSE: 0.8855\n",
      "Epoch 800/1000 — Train MSE: 0.8869\n",
      "Epoch 900/1000 — Train MSE: 0.8839\n",
      "Test MSE: 0.8805722162049375\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionGD(regularization=True, regularization_method='Elastic')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dbaeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SGDRegressor\": SGDRegressor(loss='squared_error', random_state=42),\n",
    "    \"Ridge\": Ridge(solver='saga'),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c54591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor Test MSE: 0.5506\n",
      "Ridge      Test MSE: 0.5559\n",
      "Lasso      Test MSE: 1.3107\n",
      "ElasticNet Test MSE: 1.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kashyappatel/anaconda3/envs/ds605/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name:10s} Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05140ccd",
   "metadata": {},
   "source": [
    "## Linear Regression with Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bc9e9",
   "metadata": {},
   "source": [
    "### OLS (Ordinary Least Squares):\n",
    "\n",
    "$\n",
    "J(\\theta) \\;=\\; \\frac{1}{\\text{m}}\\,\\|y - X\\theta\\|_2^2 \\;=\\; \\frac{1}{\\text{m}}(y - X\\theta)^\\top (y - X\\theta)\n",
    "$\n",
    "\n",
    "$\n",
    "J(\\theta)=\\frac{1}{\\text{m}}\\big(y^\\top y - 2 y^\\top X\\theta + \\theta^\\top X^\\top X \\theta\\big)\n",
    "$\n",
    "\n",
    "* Gradient of $y^\\top y$ wrt $\\theta$ is 0.\n",
    "* Gradient of $2 y^\\top X\\theta$ wrt $\\theta$ is $2 X^\\top y$.\n",
    "    * $\\nabla_\\theta (A^\\top \\theta) = A$\n",
    "* Gradient of $\\theta^\\top X^\\top X \\theta$ wrt $\\theta$ is $2 X^\\top X \\theta$.\n",
    "    * $\\nabla_\\theta (\\theta^\\top A \\theta) = (A + A^\\top)\\theta$\n",
    "    * if $A$ is symmetric ($X^\\top X$ is symmetric) then $\\nabla_\\theta (\\theta^\\top A \\theta) = 2 A \\theta$\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{\\text{m}}X^\\top\\big(X \\theta - y \\big)\n",
    "$\n",
    "\n",
    "$\n",
    "-X^\\top y + X^\\top X \\theta = 0 \\quad\\Longrightarrow\\quad X^\\top X \\theta = X^\\top y\n",
    "$\n",
    "\n",
    "$\n",
    "\\theta_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y\\\n",
    "$\n",
    "\n",
    "If $X^\\top X$ is singular or nearly singular, replace inverse with the Moore–Penrose pseudoinverse:\n",
    "\n",
    "$\n",
    "\\theta_{\\text{OLS}} = X^\\dagger y$, where $X^\\dagger = (X^\\top X)^\\dagger X^\\top\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Ridge Regression:\n",
    "\n",
    "$\n",
    "J(\\theta) \\;=\\; \\frac{1}{\\text{m}}\\|y - X\\theta\\|_2^2 \\;+\\; \\lambda\\,\\|\\theta\\|_2^2\n",
    "\\;=\\; \\frac{1}{\\text{m}}(y - X\\theta)^\\top (y - X\\theta) + \\lambda\\theta^\\top\\theta\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{\\text{m}}X^\\top\\big(X \\theta - y \\big) + 2 \\lambda \\theta\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{2}{\\text{m}}X^\\top\\big(X \\theta - y \\big) + 2 \\lambda \\theta = 0 \\quad\\Longrightarrow\\quad (\\frac{1}{\\text{m}}X^\\top X + \\lambda I)\\theta = \\frac{1}{\\text{m}}X^\\top y\n",
    "$\n",
    "\n",
    "$\n",
    "I' = \\begin{bmatrix} 0 & 0 \\\\ 0 & I_{d-1} \\end{bmatrix},\\quad\n",
    "\\theta = (X^\\top X + \\text{m}\\lambda I')^{-1} X^\\top y\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ca42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNE:\n",
    "    def __init__(self, regularization=None, lambda_=1.0):\n",
    "        \"\"\"\n",
    "        regularization: \n",
    "            - None  -> OLS\n",
    "            - 'L2'  -> Ridge\n",
    "        lambda_: regularization strength\n",
    "        \"\"\"\n",
    "        self.regularization = regularization\n",
    "        self.lambda_ = lambda_\n",
    "        self.theta = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X_b = np.hstack([ones, X])\n",
    "\n",
    "        n_features = X_b.shape[1]\n",
    "        m = X_b.shape[0]\n",
    "\n",
    "        if self.regularization is None:\n",
    "            self.theta = np.linalg.pinv(X_b.T @ X_b) @ (X_b.T @ y)\n",
    "\n",
    "        elif self.regularization == 'L2':\n",
    "            I = np.eye(n_features)\n",
    "            I[0, 0] = 0\n",
    "            self.theta = np.linalg.pinv(X_b.T @ X_b + m * self.lambda_ * I) @ (X_b.T @ y)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Only 'None' (OLS) and 'L2' (Ridge) are supported.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X_b = np.hstack([ones, X])\n",
    "        return X_b @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6cc298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.5558915986952413\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionNE()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8933e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.8021077157578121\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionNE(regularization='L2')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f57e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"OLS\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(solver='cholesky')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6854acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS        Test MSE: 0.5559\n",
      "Ridge      Test MSE: 0.5559\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name:10s} Test MSE: {mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
